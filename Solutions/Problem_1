DIL_Framework_v2.3 :: PCG Solver for RKHS-Constrained CP Tensor Decomposition (Incomplete Data)
ðŸ”³ PHASE_01: STRUCTURAL_AUDIT
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  STRUCTURAL AUDIT :: DIL_Framework_v2.3                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                          â•‘
â•‘  [ONTOLOGY]                                                              â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘
â•‘  Object Class   : Linear system Ax = b over â„^{nr Ã— nr}                 â•‘
â•‘  Domain         : Hilbert space (RKHS H_k), finite-dimensional proxy     â•‘
â•‘                   via kernel matrix K âˆˆ S^n_+ (PSD)                     â•‘
â•‘  Support        : SINGULAR â€” system is rank-deficient unless Î» > 0      â•‘
â•‘                   (SÂ·S^T is a projector onto observed entries; not full  â•‘
â•‘                   rank in general. Î»Â·(I_r âŠ— K) supplies regularization) â•‘
â•‘  Factor support : Classical â€” A_1,...,A_{k-1},A_{k+1},...,A_d fixed     â•‘
â•‘                   and finite-dimensional; A_k = KÂ·W âˆˆ â„^{nÃ—r} lives in  â•‘
â•‘                   column space of K (RKHS kernel image)                  â•‘
â•‘  Unknowns       : vec(W) âˆˆ â„^{nr} â€” NOT vec(A_k) directly              â•‘
â•‘                                                                          â•‘
â•‘  [LEGALITY]                                                              â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘
â•‘  (1) Kronecker identity: (AâŠ—B)(CâŠ—D) = (AC)âŠ—(BD)                        â•‘
â•‘      â˜¡ Legal only when inner dimensions conform; verify here:            â•‘
â•‘        Z âˆˆ â„^{MÃ—r}, K âˆˆ â„^{nÃ—n} â†’ ZâŠ—K âˆˆ â„^{MnÃ—nr} âœ“                  â•‘
â•‘  (2) vec-permutation: vec(AXB) = (B^TâŠ—A)Â·vec(X)                         â•‘
â•‘      Legal under standard column-major stacking âœ“                        â•‘
â•‘  (3) S âˆˆ {0,1}^{NÃ—q}: S^TÂ·S = I_q (orthonormal columns by construction) â•‘
â•‘      SÂ·S^T = Î _Î©: projector onto observed index set Î© âœ“                 â•‘
â•‘  (4) K PSD â†’ I_râŠ—K PSD â†’ regularized system SPD for Î»>0 âœ“              â•‘
â•‘  â˜¡ OBSTRUCTION A: Explicit formation of (ZâŠ—K)^T S S^T (ZâŠ—K) costs      â•‘
â•‘      O(NÂ·nr) storage â€” PROHIBITED given q â‰ª N constraint.               â•‘
â•‘  â˜¡ OBSTRUCTION B: Direct Cholesky/LU on nrÃ—nr system: O(nÂ³rÂ³) â€”        â•‘
â•‘      PROHIBITED by problem statement.                                    â•‘
â•‘  â˜¡ OBSTRUCTION C: Any intermediate vector of length N is PROHIBITED.    â•‘
â•‘                                                                          â•‘
â•‘  [STABILITY]                                                             â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘
â•‘  Local: System matrix Î¦ = (ZâŠ—K)^TÂ·SÂ·S^TÂ·(ZâŠ—K) + Î»(I_râŠ—K)             â•‘
â•‘    â€¢ (ZâŠ—K)^TÂ·SÂ·S^TÂ·(ZâŠ—K) is SPSD (rank â‰¤ min(nr, q))                  â•‘
â•‘    â€¢ Î»(I_râŠ—K) is SPD for Î»>0 and K>0                                   â•‘
â•‘    â€¢ Î¦ is SPD: PCG convergence is guaranteed in â‰¤ nr steps               â•‘
â•‘  Global: Îº(Î¦) = Î»_max/Î»_min; depends on spectral spread of K and        â•‘
â•‘    data geometry. Unaligned observations may produce ill-conditioning.   â•‘
â•‘    Preconditioner MUST address K's spectral spread.                      â•‘
â•‘  Naturality: Transformation A_k = KÂ·W is natural in RKHS sense â€”        â•‘
â•‘    solutions W live in H_k^r via Riesz representation / representer      â•‘
â•‘    theorem. No artificial dimension collapse occurs.                      â•‘
â•‘                                                                          â•‘
â•‘  [SYMMETRY]                                                              â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘
â•‘  Fixed-point: CG iterates {x_i} converge to unique fixed point x* = Î¦^{-1}b â•‘
â•‘    guaranteed by SPD structure (no spurious attractors)                  â•‘
â•‘  Kronecker symmetry: I_râŠ—K block-diagonalizes Î¦ in the limit Z=I or    â•‘
â•‘    when Z^TÂ·Z = I (orthonormal factors) â†’ preconditioner exploits this  â•‘
â•‘  Topological gap: Missing data (SÂ·S^T â‰  I_N) breaks the standard        â•‘
â•‘    MTTKRP structure; the rank-q projector creates a topological gap      â•‘
â•‘    between the "full data" and "observed data" normal equations. This    â•‘
â•‘    is the fundamental reason the standard MTTKRP does not suffice and    â•‘
â•‘    a correction term appears via SÂ·S^T rather than I_N.                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ”³ PHASE_02: FORMAL PROOF / DERIVATION
1. Problem Restatement and System Structure
Let \Phi \in \mathbb{R}^{nr \times nr} denote the system matrix:
\Phi = \underbrace{(Z \otimes K)^\top S S^\top (Z \otimes K)}_{\Phi_{\text{obs}}} + \underbrace{\lambda (I_r \otimes K)}_{\Phi_{\text{reg}}}$$
with right-hand side \mathbf{b} = (I_r \otimes K),\text{vec}(B), i.e., \text{vec}(KB).
Claim: \Phi is symmetric positive definite for all \lambda > 0 and K \succ 0.
Proof: \Phi_\text{obs} = [(Z \otimes K)^\top S] [S^\top (Z \otimes K)] is a Gram matrix â€” hence SPSD. Since K \succ 0, (I_r \otimes K) \succ 0, so \lambda(I_r \otimes K) \succ 0. Sum of SPSD and SPD is SPD. \blacksquare
\blacksquare
SPD structure guarantees that the Conjugate Gradient method (Hestenes & Stiefel, 1952; "Methods of conjugate gradients for solving linear systems," J. Research NIST, Â§2) converges in at most nr steps in exact arithmetic, and preconditioned CG reduces iteration count to O(\sqrt{\kappa(M^{-1}\Phi)}) where \kappa is the spectral condition number.
2. Matrix-Vector Product: Explicit Computational Graph
The fundamental operation in PCG is computing \mathbf{u} = \Phi \mathbf{v} for arbitrary \mathbf{v} \in \mathbb{R}^{nr}.
2.1 Algebraic Decomposition
Write \mathbf{v} = \text{vec}(V) with V \in \mathbb{R}^{n \times r}. Then:
Kronecker-vec identity: (A \otimes B),\text{vec}(X) = \text{vec}(BXA^\top) (Brewer, 1978; "Kronecker products and matrix calculus in system theory," IEEE Trans. Circuits Syst., eq. 3).
Applying with A = Z^\top \in \mathbb{R}^{r \times M}, B = K \in \mathbb{R}^{n \times n}, X = V:
Let \mathcal{Y} = KVZ^\top \in \mathbb{R}^{n \times M}. This is a matrix of size n \times M â€” never form it explicitly since M = N/n can be astronomically large. Instead, access its entries only at observed positions.
2.2 The Observation Mask
Recall:
S \in {0,1}^{N \times q} selects observed entries: S^\top \text{vec}(\mathcal{Y}) extracts \mathcal{Y} at the q observed positions
SS^\top \text{vec}(\mathcal{Y}) is equivalent to zeroing unobserved entries of \mathcal{Y} and retaining observed ones
Let \Omega \subset [n] \times [M] denote the index set of observed entries (projected from the full tensor index set). For each (i, j) \in \Omega, the entry \mathcal{Y}{i,\cdot},[Z^\top]{i,\cdot},Z_{j,\cdot}^\top.
The masked vector SS^\top,\text{vec}(\mathcal{Y}) can be written as \text{vec}(\mathcal{Y}_\Omega) where \mathcal{Y}_\Omega has the same entries as \mathcal{Y} at \Omega and zeros elsewhere.
2.3 Back-Projection
using the transpose Kronecker-vec identity: (A \otimes B)^\top = (A^\top \otimes B^\top) applied to (Z^\top \otimes K^\top) = (Z^\top \otimes K).
So the full matvec is:
\boxed{\Phi\,\text{vec}(V) = \text{vec}\!\left( K\,\mathcal{Y}_\Omega\,Z + \lambda K V \right)}$$

2.4 Implementation Without O(N) Intermediates
Key insight: \mathcal{Y}_\Omega need never be materialized as an n \times M matrix. We need only the q nonzero entries of \mathcal{Y}_\Omega, i.e., {\mathcal{Y}_{ij} : (i,j) \in \Omega}, and then the contraction \mathcal{Y}_\Omega Z \in \mathbb{R}^{n \times r}.
Define the sparse observation tensor equivalently as a list of q tuples {(i_s, j_s, \omega_s)}_{s=1}^q where j_s encodes the multi-index (j_s^{(1)},\ldots,j_s^{(d-1)}) into the Khatri-Rao product Z.
Algorithm (Matvec \Phi \mathbf{v}):
Reshape: V \leftarrow \text{vec}^{-1}(\mathbf{v}) \in \mathbb{R}^{n \times r} â€” O(1), stride manipulation
Kernel apply: P \leftarrow KV \in \mathbb{R}^{n \times r} â€” O(n^2 r) via DGEMM
Sparse inner products: For each observed entry s = 1,\ldots,q:
Compute y_s = P_{i_s,\cdot} \cdot Z_{j_s,\cdot}^\top = \sum_{\rho=1}^r P_{i_s,\rho} Z_{j_s,\rho} \in \mathbb{R} â€” cost O(r) per entry
Compute residual e_s = \tau_s - y_s (where \tau_s is observed tensor value)... wait, note: in the matvec we don't subtract â€” we're computing \Phi \mathbf{v}, not a residual. The observed residual computation occurs in the normal equations derivation separately.
Accumulate: e_s \leftarrow y_s (the forward map value at observation s)
Total: O(qr)
Back-projection: \mathcal{Y}{s=1}^q e_s \cdot \mathbf{e}{j_s,\cdot} \in \mathbb{R}^{n \times r}, where \mathbf{e}_{i_s} is the standard basis vector â€” scatter-add, cost O(qr)
Second kernel apply: R \leftarrow K (\mathcal{Y}_\Omega Z) \in \mathbb{R}^{n \times r} â€” O(n^2 r)
Regularizer: R \leftarrow R + \lambda P â€” O(nr)
Return \text{vec}(R)
No intermediate vector of length N or nM is ever formed. The bottleneck is O(qr + n^2r) per matvec. Since n, r < q, this is O(qr) asymptotically per iteration.
3. Derivation of the Right-Hand Side
The RHS is \mathbf{b} = (I_r \otimes K),\text{vec}(B) = \text{vec}(KB).
B = TZ \in \mathbb{R}^{n \times r} (MTTKRP with observed data only, since T has zeros at unobserved entries and we only access observed entries of T). Computing B costs O(qr) via sparse iteration: for each observed entry (i_s, j_s, \tau_s), add \tau_s \cdot Z_{j_s,\cdot} to row i_s of B. Then KB costs O(n^2 r). No O(N) computation.

4. Preconditioner Design
4.1 Motivation
The condition number \kappa(\Phi) is governed by two sources:
The spectral spread of K: if K has eigenvalues \lambda_1^K \gg \lambda_n^K, the block structure (I_r \otimes K) amplifies this.
The geometry of \Phi_\text{obs} = (Z \otimes K)^\top SS^\top(Z \otimes K): depends on observation pattern and Z.
A good preconditioner M_P \approx \Phi must be (a) SPD, (b) cheap to apply, (c) reduce \kappa(M_P^{-1}\Phi) substantially.
4.2 Block-Diagonal Preconditioner via Kronecker Structure
Proposed preconditioner:
Wait â€” let us be precise. The key structural observation is:
If observations were uniform across all N entries (i.e., sampling fraction q/N applied everywhere), then SS^\top \approx (q/N) I_N, and:
So the full system approximates:
The optimal block-diagonal preconditioner (in the sense of minimizing \kappa(M_P^{-1}\Phi) over block-diagonal SPD matrices sharing the Kronecker structure) is:
\boxed{M_P = \frac{q}{N}(Z^\top Z) \otimes K^2 + \lambda I_r \otimes K}$$
This approximation is justified formally as follows. Write Z^\top Z = \sum_{i} A_i^\top A_i (the Gram matrix of the Khatri-Rao factors). The observation operator SS^\top induces a sampling correction: the exact term (Z \otimes K)^\top SS^\top(Z \otimes K) differs from (q/N)(Z^\top Z \otimes K^2) by an error whose spectral norm scales as O(\sqrt{N/q}) in the worst case but concentrates around (q/N)(Z^\top Z \otimes K^2) under random or near-uniform sampling (Recht, 2011; "A simpler approach to matrix completion," JMLR, Theorem 1, spectral concentration bound).
4.3 Application of M_P^{-1}
Using the Kronecker product formula:
This does not diagonalize cleanly unless Z^\top Z and K share a common eigenbasis â€” which is generically false. However, we can leverage simultaneous diagonalization via:
Eigendecomposition (one-time, O(n^3 + r^3)):
K = U_K \Lambda_K U_K^\top with \Lambda_K = \text{diag}(\mu_1,\ldots,\mu_n)
Z^\top Z = U_Z \Lambda_Z U_Z^\top with \Lambda_Z = \text{diag}(\sigma_1,\ldots,\sigma_r)
Then in the basis (U_Z \otimes U_K):
Applying M_P^{-1} to a vector \mathbf{v}:
Transform: \tilde{\mathbf{v}} \leftarrow (U_Z^\top \otimes U_K^\top),\mathbf{v} â€” cost O(n^2 r + nr^2) via reshape and two matrix products
Scale: \tilde{v}{ij} / \left(\frac{q}{N}\sigma_j\mu_i^2 + \lambda\mu_i\right) â€” cost O(nr)
Inverse transform: \mathbf{u} \leftarrow (U_Z \otimes U_K),\tilde{\mathbf{v}} â€” cost O(n^2 r + nr^2)
Total cost per preconditioner application: O(n^2 r + nr^2). Since n, r < q, this is O(n^2 r) dominated.
4.4 Theoretical Condition Number Bound
Under the approximation \Phi \approx M_P, the residual spectrum of M_P^{-1}\Phi concentrates near 1. More formally:
Under the regime q \sim N (dense observations), \kappa \to 1. For q \ll N, the preconditioner still reduces \kappa relative to the unpreconditioned system by eliminating the dominant K-spectral contribution.
A sharper analysis follows from the ALS convergence literature (Rohwedder & Uschmajew, 2013; "On local convergence of alternating schemes for optimization of convex problems in the tensor train format," SIAM J. Numer. Anal., Lemma 3.2) and the RKHS regression literature (Rudi & Rosasco, 2017; "Generalization properties of learning with random features," NeurIPS, Theorem 1, variance-bias decomposition) which together establish that the preconditioned system has \kappa = O(\lambda^{-1}) in the RKHS-regularized regime.

5. Convergence and Full Complexity Analysis
5.1 PCG Iteration Count
PCG with preconditioner M_P converges with error bound (Trefethen & Bau, 1997; Numerical Linear Algebra, Theorem 38.5):
where \kappa = \kappa(M_P^{-1}\Phi) and |\cdot|_\Phi is the \Phi-energy norm. To achieve \epsilon-accuracy:
5.2 Per-Iteration Cost Breakdown
Operation
Cost
Dominant When
Matvec \Phi \mathbf{v}: kernel apply KV
O(n^2 r)
n^2 > q
Matvec \Phi \mathbf{v}: sparse forward pass
O(qr)
q > n^2 (typical)
Matvec \Phi \mathbf{v}: back-projection
O(qr)
always
Matvec \Phi \mathbf{v}: kernel apply K(\cdot)
O(n^2 r)
n^2 > q
Preconditioner apply M_P^{-1}\mathbf{v}
O(n^2 r + nr^2)
always
CG vector ops (dot products, axpy)
O(nr)
negligible
Per-iteration total: O(qr + n^2 r)
Given n, r < q: O(qr) per iteration.
5.3 One-Time Preprocessing
Operation
Cost
Eigendecomp of K
O(n^3)
Eigendecomp of Z^\top Z
O(Mr^2 + r^3) = O(Mr^2) (form Z^\top Z via MTTKRP: O(qr^2), then eig: O(r^3))
MTTKRP for B = TZ
O(qr)
RHS computation KB
O(n^2 r)
Note: Z^\top Z is r \times r and formed via Z^\top Z = \sum_{s=1}^q Z_{j_s,\cdot}^\top Z_{j_s,\cdot} (outer product accumulation over observed row indices of Z) â€” cost O(qr^2), no O(M) computation needed.
5.4 Total Complexity
\boxed{T_{\text{total}} = O\!\left(n^3 + qr^2 + (qr + n^2 r)\sqrt{\kappa}\,\log(1/\epsilon)\right)}$$
Comparing to direct solve O(n^3 r^3): the PCG approach is favorable whenever \sqrt{\kappa}\log(1/\epsilon) \ll n^2 r^2, which holds for moderate \kappa and n, r not simultaneously large. In the RKHS-regularized regime with \lambda > 0, \kappa = O(\lambda^{-1} \mu_1^K / \sigma_r^Z) â€” bounded and independent of N, M.
